{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c0b68b-d42c-4184-99cf-535d7f51fb8c",
   "metadata": {},
   "source": [
    "# 医疗问诊意图识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793135c-5326-4b05-8e2b-560897a3e33a",
   "metadata": {},
   "source": [
    "## 安装包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ca5b277-075b-4956-b510-218bf1a636fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: transformers in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (4.15.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: sacremoses in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: requests in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from transformers) (4.62.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: joblib in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/teletraan/.pyenv/versions/3.9.4/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from torch) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from torch) (3.10.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/teletraan/.pyenv/versions/3.9.4/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting tensorboardX\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/98/88/977b2f03fd0f8a2490fc7a1ad691d5e44cee5f1dc90c57078c5c168e2e70/tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboardX) (1.20.3)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from tensorboardX) (3.17.2)\n",
      "Requirement already satisfied: six>=1.9 in /Users/teletraan/.pyenv/versions/3.9.4/lib/python3.9/site-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.4.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/teletraan/.pyenv/versions/3.9.4/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0466b-fe28-40c5-bd7e-b7a71bda8b95",
   "metadata": {},
   "source": [
    "## 读取和处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7e0a7-65b5-4793-8310-ae518edc948e",
   "metadata": {},
   "source": [
    "### 原始数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f24b443-a1bb-441c-b23d-f0b1ba02e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 训练数据\n",
    "df = pd.read_csv('train.csv')\n",
    "# 验证数据\n",
    "df_val = pd.read_csv('validation.csv')\n",
    "# 测试数据\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3be2d9-7ba9-42ed-a0e7-ee722f31da5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>您好！我爸爸七年前得过肺结核！在前几天突然咳血，血液量大，有血块！现在住院，八天了也没查出结...</td>\n",
       "      <td>临床表现(病症表现)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>喉咙痒，咳嗽，白色泡沫痰嗓子红肿怀疑是检地上的东西吃引起的，请问怎么治疗</td>\n",
       "      <td>治疗方法</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>强迫症会引起幻听吗</td>\n",
       "      <td>相关病症</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>胃涨不消化怎么办</td>\n",
       "      <td>治疗方法</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我就是经常在走路的时候出现手脚乱动，然后还有点心慌的情况，手脚就开始不停的发抖，老是有点精神...</td>\n",
       "      <td>治疗方法</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label_class  label\n",
       "0  您好！我爸爸七年前得过肺结核！在前几天突然咳血，血液量大，有血块！现在住院，八天了也没查出结...  临床表现(病症表现)      3\n",
       "1               喉咙痒，咳嗽，白色泡沫痰嗓子红肿怀疑是检地上的东西吃引起的，请问怎么治疗        治疗方法      5\n",
       "2                                          强迫症会引起幻听吗        相关病症      4\n",
       "3                                           胃涨不消化怎么办        治疗方法      5\n",
       "4  我就是经常在走路的时候出现手脚乱动，然后还有点心慌的情况，手脚就开始不停的发抖，老是有点精神...        治疗方法      5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66b2c5-0f3c-4659-be40-6f535d1607bd",
   "metadata": {},
   "source": [
    "### 样本标签分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbe2720-6582-48a5-a1a8-85d84d62b890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0      386\n",
       "1      854\n",
       "2      119\n",
       "3     1137\n",
       "4      215\n",
       "5     1802\n",
       "6       21\n",
       "7       37\n",
       "8      122\n",
       "9      321\n",
       "10      36\n",
       "11      68\n",
       "12     881\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"label\")[\"label\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f085508-2a40-4b79-a0a7-5e5de0ded92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0     13\n",
       "1     22\n",
       "2      2\n",
       "3     45\n",
       "4      6\n",
       "5     63\n",
       "6      1\n",
       "7      2\n",
       "8      5\n",
       "9     13\n",
       "11     2\n",
       "12    41\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby(\"label\")[\"label\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0b29ed-a57d-4427-98ae-1b796247fd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0      41\n",
       "1      94\n",
       "2      17\n",
       "3     131\n",
       "4      21\n",
       "5     209\n",
       "6       4\n",
       "7       4\n",
       "8      17\n",
       "9      44\n",
       "10      4\n",
       "11     10\n",
       "12     95\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby(\"label\")[\"label\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b3968-17c2-4fc5-8dcd-56748ca94a55",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ead251-8162-4ab0-a323-b8d584282830",
   "metadata": {},
   "source": [
    "#### 清除数字、特殊符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d80373c-fe52-4277-9928-d473b29a8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df.text = df.text.map(lambda x: re.sub('[a-zA-Z0-9×]*', '', x).replace(' ',''))\n",
    "df_val.text = df_val.text.map(lambda x: re.sub('[a-zA-Z0-9×]*', '', x).replace(' ',''))\n",
    "df_test.text = df_test.text.map(lambda x: re.sub('[a-zA-Z0-9×]*', '', x).replace(' ',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751df36b-0add-45e0-b568-ae38af1f6b99",
   "metadata": {},
   "source": [
    "#### 截取文本或补齐文本到固定长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37eba73-1ca0-48ec-8e1a-1e43e626f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_sentences(input_sentences, padding_token, padding_sentence_length = 230):\n",
    "    sentences = [sentence for sentence in input_sentences]\n",
    "    max_sentence_length = padding_sentence_length\n",
    "    l=[]\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > max_sentence_length:\n",
    "            sentence = sentence[:max_sentence_length]\n",
    "            l.append(sentence)\n",
    "        else:\n",
    "            sentence += padding_token * (max_sentence_length - len(sentence))\n",
    "            l.append(sentence)\n",
    "    return (l, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "832ca9fa-7911-44e6-bd1c-b995a48e5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, max_document_length = padding_sentences(df['text'], '[PAD]')\n",
    "sentences_val, _ = padding_sentences(df_val['text'], '[PAD]')\n",
    "sentences_test, _ = padding_sentences(df_test['text'], '[PAD]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087eaa8-e598-4746-84a0-c7eb6a9c7d80",
   "metadata": {},
   "source": [
    "#### 文本转为词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade276ea-d365-4713-94b9-491bbc40e945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert_chinese_tiny were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AlbertForMaskedLM, AlbertModel\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "pretrained = 'voidful/albert_chinese_tiny'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)\n",
    "albert_model = AlbertModel.from_pretrained(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9801b5e3-1254-4622-a1d5-4af0cb1df2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i,s in enumerate(sentences):\n",
    "    input_ids = torch.tensor(tokenizer.encode(s, add_special_tokens=False)).unsqueeze(0)\n",
    "    outputs = albert_model(input_ids)\n",
    "    train_data.append([outputs['last_hidden_state'].detach().numpy(), df['label'][i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78d4a3e2-22c1-4bac-ac24-784cbc81043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "for i,s in enumerate(sentences_val):\n",
    "    input_ids = torch.tensor(tokenizer.encode(s, add_special_tokens=False)).unsqueeze(0)\n",
    "    outputs = albert_model(input_ids)\n",
    "    val_data.append([outputs['last_hidden_state'].detach().numpy(), df_val['label'][i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc17bfd-d48f-450f-be95-b99f023298fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i,s in enumerate(sentences_test):\n",
    "    input_ids = torch.tensor(tokenizer.encode(s, add_special_tokens=False)).unsqueeze(0)\n",
    "    outputs = albert_model(input_ids)\n",
    "    test_data.append([outputs['last_hidden_state'].detach().numpy(), df_test['label'][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c880d8-d305-4370-8a0d-876d9c9c2d21",
   "metadata": {},
   "source": [
    "### 数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a51ab339-930f-4e6a-93ee-7a58ff35d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterater(object):\n",
    "    def __init__(self, batches, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = batches\n",
    "        self.n_batches = len(batches) // batch_size\n",
    "        self.residue = False  # 记录batch数量是否为整数\n",
    "        if len(batches) % self.n_batches != 0:\n",
    "            self.residue = True\n",
    "        self.index = 0\n",
    "        self.device = device\n",
    "\n",
    "    def _to_tensor(self, datas):\n",
    "        x = torch.FloatTensor([_[0] for _ in datas]).to(self.device)\n",
    "        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "        return x, y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.residue and self.index == self.n_batches:\n",
    "            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.residue:\n",
    "            return self.n_batches + 1\n",
    "        else:\n",
    "            return self.n_batches\n",
    "\n",
    "\n",
    "def build_iterator(dataset):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    iter = DatasetIterater(dataset, 64, device)\n",
    "    return iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d481241-736e-4c05-bb2e-4b24d6e564f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = build_iterator(train_data)\n",
    "val_iter = build_iterator(val_data) \n",
    "test_iter = build_iterator(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eae6579-1a32-495c-912b-f8f00482901b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label tensor([ 3,  5,  4,  5,  5,  5,  1, 12,  5,  5, 12,  3,  9,  5,  5,  3,  9,  5,\n",
      "        12,  3,  1,  8, 12,  3,  5,  1,  4,  3,  3,  1,  5,  9,  3,  5,  5,  0,\n",
      "         1,  1, 12,  5,  2, 11,  5,  0,  5,  3,  5,  5,  3,  5, 12,  3,  1,  5,\n",
      "         5,  5,  0, 12, 12,  7,  8, 12,  3,  5])\n"
     ]
    }
   ],
   "source": [
    "# 测试一下迭代器\n",
    "for i, (trains, labels) in enumerate(train_iter):\n",
    "    print('label',labels)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc10a49-b919-47ee-b351-c876256892de",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b1f21f3-64a2-4ae2-a41b-98daf952b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv1d(312, 256, k) for k in (2,3,4)])\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(256 * 3, 128)\n",
    "        self.fc2 = nn.Linear(128, 13)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x))\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze()\n",
    "        x = x.permute(0,2,1)\n",
    "        out = torch.cat([self.conv_and_pool(x, conv) for conv in self.convs], 1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef173c4e-f16b-4e07-9eab-eb19e9d43120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(312, 256, kernel_size=(2,), stride=(1,))\n",
       "    (1): Conv1d(312, 256, kernel_size=(3,), stride=(1,))\n",
       "    (2): Conv1d(312, 256, kernel_size=(4,), stride=(1,))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2eaf8-3f17-4f30-a3c2-fac9a443b29a",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d768d19d-aa45-479a-a6fb-5750991553d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            outputs = model(texts)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    return acc, loss_total / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91de0449-be19-4c3a-8e2d-49162e1c7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "def train(model, train_iter, val_iter, num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    val_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    writer = SummaryWriter(log_dir='./' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, num_epochs))\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            outputs = model(trains)\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if total_batch % 100 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                val_acc, val_loss = evaluate(model, val_iter)\n",
    "                if val_loss < val_best_loss:\n",
    "                    val_best_loss = val_loss\n",
    "                    torch.save(model.state_dict(), './model')\n",
    "                    improve = '*'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, val_loss, val_acc, time_dif, improve))\n",
    "                writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
    "                writer.add_scalar(\"loss/dev\", val_loss, total_batch)\n",
    "                writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
    "                writer.add_scalar(\"acc/dev\", val_acc, total_batch)\n",
    "                model.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > 1000:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        scheduler.step() # 学习率衰减\n",
    "        if flag:\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9cae34a-034e-41a9-8da7-441a4c0f6441",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "Iter:      0,  Train Loss:   2.4,  Train Acc: 17.19%,  Val Loss:   2.4,  Val Acc: 29.30%,  Time: 0:00:03 *\n",
      "Epoch [2/5]\n",
      "Iter:    100,  Train Loss:   1.4,  Train Acc: 53.12%,  Val Loss:   1.3,  Val Acc: 55.35%,  Time: 0:01:05 *\n",
      "Epoch [3/5]\n",
      "Iter:    200,  Train Loss:   1.2,  Train Acc: 56.25%,  Val Loss:   1.2,  Val Acc: 58.14%,  Time: 0:02:03 *\n",
      "Epoch [4/5]\n",
      "Iter:    300,  Train Loss:   1.0,  Train Acc: 57.81%,  Val Loss:   1.1,  Val Acc: 60.47%,  Time: 0:03:01 *\n",
      "Epoch [5/5]\n",
      "Iter:    400,  Train Loss:  0.65,  Train Acc: 82.81%,  Val Loss:   1.2,  Val Acc: 58.14%,  Time: 0:03:59 \n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, val_iter, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d3eea-64d9-4703-bda8-380e15ef61ec",
   "metadata": {},
   "source": [
    "## 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7339a202-f699-4a23-a1f3-73ef42bead83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_iter):\n",
    "    # model.load_state_dict(torch.load(config.save_path))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecdd01e0-83c8-4bed-b4e8-0191730e424b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:   1.2,  Test Acc: 59.48%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5200    0.6341    0.5714        41\n",
      "           1     0.5849    0.6596    0.6200        94\n",
      "           2     0.6250    0.5882    0.6061        17\n",
      "           3     0.5393    0.3664    0.4364       131\n",
      "           4     0.4000    0.2857    0.3333        21\n",
      "           5     0.7131    0.8086    0.7578       209\n",
      "           6     1.0000    0.2500    0.4000         4\n",
      "           7     0.0000    0.0000    0.0000         4\n",
      "           8     0.8571    0.3529    0.5000        17\n",
      "           9     0.8333    0.3409    0.4839        44\n",
      "          10     0.6667    0.5000    0.5714         4\n",
      "          11     1.0000    0.4000    0.5714        10\n",
      "          12     0.4336    0.6526    0.5210        95\n",
      "\n",
      "    accuracy                         0.5948       691\n",
      "   macro avg     0.6287    0.4492    0.4902       691\n",
      "weighted avg     0.6138    0.5948    0.5839       691\n",
      "\n",
      "Confusion Matrix...\n",
      "[[ 26   6   0   7   1   0   0   0   0   0   0   0   1]\n",
      " [  7  62   0   9   1   7   0   1   0   0   0   0   7]\n",
      " [  1   0  10   1   1   3   0   0   0   1   0   0   0]\n",
      " [ 11  24   1  48   3  21   0   0   0   0   0   0  23]\n",
      " [  3   2   0   6   6   2   0   0   0   0   0   0   2]\n",
      " [  1   5   3   7   1 169   0   0   1   0   0   0  22]\n",
      " [  0   0   0   0   0   2   1   0   0   0   1   0   0]\n",
      " [  1   1   0   0   1   0   0   0   0   0   0   0   1]\n",
      " [  0   1   0   1   0   6   0   1   6   0   0   0   2]\n",
      " [  0   1   1   0   1   7   0   0   0  15   0   0  19]\n",
      " [  0   0   0   0   0   0   0   0   0   0   2   0   2]\n",
      " [  0   1   1   0   0   2   0   0   0   0   0   4   2]\n",
      " [  0   3   0  10   0  18   0   0   0   2   0   0  62]]\n",
      "Time usage: 0:00:05\n"
     ]
    }
   ],
   "source": [
    "test(model, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b009735-dc1b-43e5-b9bf-0043f1f4be7c",
   "metadata": {},
   "source": [
    "## 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "427ebe74-62eb-4acf-9ec6-57a50d89202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ed2e9-8650-407a-81a6-601f80773f8d",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2474c-ff76-496f-b150-6d0c7ec008b6",
   "metadata": {},
   "source": [
    "数据的标签分布不均，最终的模型出现了过拟合，后续可通过增加数据，变更或调整模型来进行优化。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
